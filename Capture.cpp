/* -LICENSE-START-
** Copyright (c) 2013 Blackmagic Design
**  
** Permission is hereby granted, free of charge, to any person or organization 
** obtaining a copy of the software and accompanying documentation (the 
** "Software") to use, reproduce, display, distribute, sub-license, execute, 
** and transmit the Software, and to prepare derivative works of the Software, 
** and to permit third-parties to whom the Software is furnished to do so, in 
** accordance with:
** 
** (1) if the Software is obtained from Blackmagic Design, the End User License 
** Agreement for the Software Development Kit (“EULA”) available at 
** https://www.blackmagicdesign.com/EULA/DeckLinkSDK; or
** 
** (2) if the Software is obtained from any third party, such licensing terms 
** as notified by that third party,
** 
** and all subject to the following:
** 
** (3) the copyright notices in the Software and this entire statement, 
** including the above license grant, this restriction and the following 
** disclaimer, must be included in all copies of the Software, in whole or in 
** part, and all derivative works of the Software, unless such copies or 
** derivative works are solely in the form of machine-executable object code 
** generated by a source language processor.
** 
** (4) THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS 
** OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
** FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT 
** SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE 
** FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, 
** ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER 
** DEALINGS IN THE SOFTWARE.
** 
** A copy of the Software is available free of charge at 
** https://www.blackmagicdesign.com/desktopvideo_sdk under the EULA.
** 
** -LICENSE-END-
*/

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <pthread.h>
#include <unistd.h>
#include <fcntl.h>
#include <csignal>
#include <vector>
#include <deque>
#include <numeric>
#include <sstream>

#include <websocketpp/config/asio_no_tls_client.hpp>
#include <websocketpp/client.hpp>

#include "DeckLinkAPI.h"
#include "Capture.h"
#include "Config.h"
#include "LKFS.h"
#include "avectorscope_processor.h" // Renamed for clarity
#include "eq_processor.h"           // For EQ Meter processing

// WebSocket client
typedef websocketpp::client<websocketpp::config::asio_client> client;
using websocketpp::lib::bind;
using websocketpp::lib::placeholders::_1, websocketpp::lib::placeholders::_2;

static client g_ws_client;
static websocketpp::connection_hdl g_ws_hdl;
static bool g_ws_connected = false;
static pthread_mutex_t g_ws_mutex;
static pthread_t g_ws_thread;
static std::string g_ws_uri = "ws://127.0.0.1:8080";
static bool g_ws_started = false;
static bool		 g_do_exit = false;
static bool g_isIntegrating = false;

// FFmpeg-related globals are now encapsulated in AVectorscopeProcessor
static AVectorscopeProcessor g_avectorscopeProcessor;
static EQProcessor g_eqProcessor;

// Audio processing globals
std::deque<double> g_leftChannelPcm;
std::deque<double> g_rightChannelPcm;
std::deque<double> g_shortTermLeftChannelPcm;
std::deque<double> g_shortTermRightChannelPcm;
std::vector<double> g_momentaryLoudnessHistory;



const int kAudioSampleRate = 48000;
const int kWindowSizeInSamples = kAudioSampleRate * 400 / 1000; // 19200
const int kShortTermWindowSizeInSamples = kAudioSampleRate * 3; // 144000
const int kSlideSizeInSamples = kAudioSampleRate * 100 / 1000;  // 4800

void* ws_thread_func(void* /*arg*/)
{
    try {
        g_ws_client.run();
    } catch (const std::exception & e) {
        fprintf(stderr, "WebSocket thread exception: %s\n", e.what());
    } catch (websocketpp::lib::error_code e) {
        fprintf(stderr, "WebSocket thread error: %s\n", e.message().c_str());
    } catch (...) {
        fprintf(stderr, "WebSocket thread unknown exception.\n");
    }
    return NULL;
}

void send_ws_message(const std::string& msg) {
    if (g_do_exit) return;
    pthread_mutex_lock(&g_ws_mutex);
    if (g_ws_connected) {
        websocketpp::lib::error_code ec;
        g_ws_client.send(g_ws_hdl, msg, websocketpp::frame::opcode::text, ec);
        if (ec) {
            fprintf(stderr, "WebSocket send failed: %s\n", ec.message().c_str());
        }
    }
    pthread_mutex_unlock(&g_ws_mutex);
}

void on_ws_open(client* c, websocketpp::connection_hdl hdl) {
    pthread_mutex_lock(&g_ws_mutex);
    g_ws_connected = true;
    g_ws_hdl = hdl;
    pthread_mutex_unlock(&g_ws_mutex);
    fprintf(stderr, "WebSocket connection opened.\n");
    fflush(stderr);
}

void on_ws_close(client* c, websocketpp::connection_hdl hdl) {
    pthread_mutex_lock(&g_ws_mutex);
    g_ws_connected = false;
    pthread_mutex_unlock(&g_ws_mutex);
    fprintf(stderr, "WebSocket connection closed.\n");
    fflush(stderr);
}

void on_ws_message(client* c, websocketpp::connection_hdl hdl, client::message_ptr msg) {
    std::string payload = msg->get_payload();
    // A more robust check that is insensitive to whitespace around the colon
    if (payload.find("\"command\"") != std::string::npos && payload.find("\"start_integration\"") != std::string::npos) {
        fprintf(stderr, "Received start integration command.\n");
        g_momentaryLoudnessHistory.clear();
        g_isIntegrating = true;
    } else if (payload.find("\"command\"") != std::string::npos && payload.find("\"stop_integration\"") != std::string::npos) {
        fprintf(stderr, "Received stop integration command.\n");
        g_isIntegrating = false;
    }
}

static pthread_mutex_t	 g_sleepMutex;
static pthread_cond_t	 g_sleepCond;

static BMDConfig		 g_config;

static IDeckLinkInput*		 g_deckLinkInput = NULL;

DeckLinkCaptureDelegate::DeckLinkCaptureDelegate() :
	m_refCount(1)
{
}

ULONG DeckLinkCaptureDelegate::AddRef(void)
{
	return __sync_add_and_fetch(&m_refCount, 1);
}

ULONG DeckLinkCaptureDelegate::Release(void)
{
	int32_t newRefValue = __sync_sub_and_fetch(&m_refCount, 1);
	if (newRefValue == 0)
	{
		delete this;
		return 0;
	}
	return newRefValue;
}

HRESULT DeckLinkCaptureDelegate::VideoInputFrameArrived(IDeckLinkVideoInputFrame* videoFrame, IDeckLinkAudioInputPacket* audioFrame)
{
    (void)videoFrame; // Video frames are ignored, but used for timing

    if (audioFrame)
    {
        void* audioFrameBytes;
        audioFrame->GetBytes(&audioFrameBytes);
        const unsigned int sampleFrameCount = audioFrame->GetSampleFrameCount();
        const unsigned int channelCount = g_config.m_audioChannels;
        const unsigned int sampleDepth = g_config.m_audioSampleDepth;

        if (channelCount == 2)
        {
            // 1a. Calculate peak levels for the current audio frame and send to websocket
            double maxLeft = 0.0;
            double maxRight = 0.0;

            // Temporary storage for the current audio frame's samples for EQ processing
            std::vector<double> current_left_samples;
            std::vector<double> current_right_samples;
            current_left_samples.reserve(sampleFrameCount);
            current_right_samples.reserve(sampleFrameCount);

            // 1b. Append new samples to global buffers.
            if (sampleDepth == 32)
            {
                int32_t* pcmData = (int32_t*)audioFrameBytes;
                for (unsigned int i = 0; i < sampleFrameCount; ++i)
                {
                    double leftSample = (double)pcmData[i * 2] / 2147483648.0;
                    double rightSample = (double)pcmData[i * 2 + 1] / 2147483648.0;

                    if (std::abs(leftSample) > maxLeft) maxLeft = std::abs(leftSample);
                    if (std::abs(rightSample) > maxRight) maxRight = std::abs(rightSample);

                    g_leftChannelPcm.push_back(leftSample);
                    g_rightChannelPcm.push_back(rightSample);
                    g_shortTermLeftChannelPcm.push_back(leftSample);
                    g_shortTermRightChannelPcm.push_back(rightSample);
                    current_left_samples.push_back(leftSample);
                    current_right_samples.push_back(rightSample);
                }
            }
            else if (sampleDepth == 16)
            {
                int16_t* pcmData = (int16_t*)audioFrameBytes;
                for (unsigned int i = 0; i < sampleFrameCount; ++i)
                {
                    double leftSample = (double)pcmData[i * 2] / 32768.0;
                    double rightSample = (double)pcmData[i * 2 + 1] / 32768.0;

                    if (std::abs(leftSample) > maxLeft) maxLeft = std::abs(leftSample);
                    if (std::abs(rightSample) > maxRight) maxRight = std::abs(rightSample);

                    g_leftChannelPcm.push_back(leftSample);
                    g_rightChannelPcm.push_back(rightSample);
                    g_shortTermLeftChannelPcm.push_back(leftSample);
                    g_shortTermRightChannelPcm.push_back(rightSample);
                    current_left_samples.push_back(leftSample);
                    current_right_samples.push_back(rightSample);
                }
            }

            double leftDb = (maxLeft > 0.0) ? (20.0 * log10(maxLeft)) : -100.0;
            double rightDb = (maxRight > 0.0) ? (20.0 * log10(maxRight)) : -100.0;
            std::ostringstream oss_levels;
            oss_levels << "{\"type\": \"levels\", \"left\": " << leftDb << ", \"right\": " << rightDb << "}";
            send_ws_message(oss_levels.str());

            // 2. Immediate vectorscope processing
            if (sampleFrameCount > 0)
            {
                std::vector<float> leftChunk(g_leftChannelPcm.end() - sampleFrameCount, g_leftChannelPcm.end());
                std::vector<float> rightChunk(g_rightChannelPcm.end() - sampleFrameCount, g_rightChannelPcm.end());
                g_avectorscopeProcessor.processAudio(leftChunk, rightChunk, sampleFrameCount,
                    [](const std::string& msg) {
                        send_ws_message(msg);
                    }
                );
            }

            // 3. Momentary and Integrated loudness calculation
            {
                while (g_leftChannelPcm.size() >= kWindowSizeInSamples)
                {
                    std::vector<double> leftWindow(g_leftChannelPcm.begin(), g_leftChannelPcm.begin() + kWindowSizeInSamples);
                    std::vector<double> rightWindow(g_rightChannelPcm.begin(), g_rightChannelPcm.begin() + kWindowSizeInSamples);
                    double lkfs = Momentary_loudness(leftWindow, rightWindow, kAudioSampleRate);
                    std::ostringstream oss;
                    oss << "{\"type\": \"lkfs\", \"value\": " << lkfs << "}";
                    send_ws_message(oss.str());

                    if(g_isIntegrating){
                        g_momentaryLoudnessHistory.push_back(lkfs);
                        double i_lkfs = integrated_loudness_with_momentaries(g_momentaryLoudnessHistory, kAudioSampleRate);
                        std::ostringstream oss_i;
                        oss_i << "{\"type\": \"i_lkfs\", \"value\": " << i_lkfs << "}";
                        send_ws_message(oss_i.str());
                    }

                    for (unsigned int i = 0; i < kSlideSizeInSamples; ++i)
                    {
                        g_leftChannelPcm.pop_front();
                        g_rightChannelPcm.pop_front();
                    }
                }
            }

            // 4. Short-term loudness calculation
            {
                while (g_shortTermLeftChannelPcm.size() >= kShortTermWindowSizeInSamples)
                {
                    std::vector<double> leftWindow(g_shortTermLeftChannelPcm.begin(), g_shortTermLeftChannelPcm.begin() + kShortTermWindowSizeInSamples);
                    std::vector<double> rightWindow(g_shortTermRightChannelPcm.begin(), g_shortTermRightChannelPcm.begin() + kShortTermWindowSizeInSamples);
                    double s_lkfs = ShortTerm_loudness(leftWindow, rightWindow, kAudioSampleRate);
                    std::ostringstream oss_s;
                    oss_s << "{\"type\": \"s_lkfs\", \"value\": " << s_lkfs << "}";
                    send_ws_message(oss_s.str());

                    for (unsigned int i = 0; i < kSlideSizeInSamples; ++i)
                    {
                        g_shortTermLeftChannelPcm.pop_front();
                        g_shortTermRightChannelPcm.pop_front();
                    }
                }
            }

            // 5. FFT for EQ Meter
            if (sampleFrameCount > 0)
            {
                g_eqProcessor.processAudio(current_left_samples.data(), current_right_samples.data(), sampleFrameCount,
                    [](const std::string& msg) {
                        send_ws_message(msg);
                    }
                );
            }
        }
    }
    return S_OK;
}


HRESULT DeckLinkCaptureDelegate::VideoInputFormatChanged(BMDVideoInputFormatChangedEvents /*events*/, IDeckLinkDisplayMode* /*mode*/, BMDDetectedVideoInputFormatFlags /*formatFlags*/)
{
	return S_OK;
}

static void sigfunc(int signum)
{
	if (signum == SIGINT || signum == SIGTERM)
			 g_do_exit = true;

	pthread_cond_signal(&g_sleepCond);
}

int main(int argc, char *argv[])
{
	HRESULT			 result;
	int			 exitStatus = 1;

	IDeckLinkIterator*		 deckLinkIterator = NULL;
	IDeckLink*			 deckLink = NULL;

	IDeckLinkProfileAttributes*	 deckLinkAttributes = NULL;
	int64_t			 duplexMode;

	IDeckLinkDisplayMode*		 displayMode = NULL;

	DeckLinkCaptureDelegate*		 delegate = NULL;

	pthread_mutex_init(&g_sleepMutex, NULL);
	pthread_cond_init(&g_sleepCond, NULL);
	pthread_mutex_init(&g_ws_mutex, NULL);

	signal(SIGINT, sigfunc);
	signal(SIGTERM, sigfunc);
	signal(SIGHUP, sigfunc);

	if (!g_config.ParseArguments(argc, argv))
	{
		g_config.DisplayUsage(exitStatus);
		goto bail;
	}

    // WebSocket client setup
    try {
        g_ws_client.clear_access_channels(websocketpp::log::alevel::all);
        g_ws_client.set_access_channels(websocketpp::log::alevel::connect | websocketpp::log::alevel::disconnect);
        g_ws_client.set_error_channels(websocketpp::log::elevel::all);
        g_ws_client.init_asio();

        g_ws_client.set_open_handler(bind(&on_ws_open, &g_ws_client, ::_1));
        g_ws_client.set_close_handler(bind(&on_ws_close, &g_ws_client, ::_1));
        g_ws_client.set_message_handler(bind(&on_ws_message, &g_ws_client, ::_1, ::_2));

        websocketpp::lib::error_code ec;
        client::connection_ptr con = g_ws_client.get_connection(g_ws_uri, ec);
        if (ec) {
            fprintf(stderr, "Could not create connection: %s\n", ec.message().c_str());
            goto bail;
        }

        g_ws_client.connect(con);

        pthread_create(&g_ws_thread, NULL, ws_thread_func, NULL);
        g_ws_started = true;

    } catch (const std::exception & e) {
        fprintf(stderr, "WebSocket setup exception: %s\n", e.what());
        goto bail;
    } catch (websocketpp::lib::error_code e) {
        fprintf(stderr, "WebSocket setup error: %s\n", e.message().c_str());
        goto bail;
    } catch (...) {
        fprintf(stderr, "WebSocket setup unknown exception.\n");
        goto bail;
    }

	if (!g_avectorscopeProcessor.initialize()) {
		fprintf(stderr, "Failed to initialize vectorscope processor\n");
		goto bail;
	}
	g_eqProcessor.initialize();

	deckLink = g_config.GetSelectedDeckLink();
	if (deckLink == NULL)
	{
		fprintf(stderr, "Unable to get DeckLink device %u\n", g_config.m_deckLinkIndex);
		goto bail;
	}

	result = deckLink->QueryInterface(IID_IDeckLinkInput, (void**)&g_deckLinkInput);
	if (result != S_OK)
	{
		fprintf(stderr, "The selected device does not have an input interface\n");
		goto bail;
	}

	result = deckLink->QueryInterface(IID_IDeckLinkProfileAttributes, (void**)&deckLinkAttributes);
	if (result != S_OK)
	{
		fprintf(stderr, "Unable to get DeckLink attributes interface\n");
		goto bail;
	}
	result = deckLinkAttributes->GetInt(BMDDeckLinkDuplex, &duplexMode);
	if ((result != S_OK) || (duplexMode == bmdDuplexInactive))
	{
		fprintf(stderr, "The selected DeckLink device is inactive\n");
		goto bail;
	}

    bool formatDetectionSupported;
    result = deckLinkAttributes->GetFlag(BMDDeckLinkSupportsInputFormatDetection, &formatDetectionSupported);
    if (result == S_OK && formatDetectionSupported)
    {
        g_config.m_inputFlags |= bmdVideoInputEnableFormatDetection;
    }

    displayMode = g_config.GetSelectedDeckLinkDisplayMode(deckLink);
    if (displayMode == NULL)
    {
        fprintf(stderr, "Error: Could not find a valid display mode.\n");
        goto bail;
    }

	delegate = new DeckLinkCaptureDelegate();
	g_deckLinkInput->SetCallback(delegate);

    result = g_deckLinkInput->EnableVideoInput(displayMode->GetDisplayMode(), bmdFormat8BitYUV, g_config.m_inputFlags);
    if (result != S_OK)
    {
        fprintf(stderr, "Failed to enable video input. Is a video signal connected?\n");
        goto bail;
    }

    result = g_deckLinkInput->EnableAudioInput(bmdAudioSampleRate48kHz, g_config.m_audioSampleDepth, g_config.m_audioChannels);
    if (result != S_OK)
    {
        fprintf(stderr, "Failed to enable audio input.\n");
        goto bail;
    }

    result = g_deckLinkInput->StartStreams();
    if (result != S_OK)
    {
        fprintf(stderr, "Failed to start streams.\n");
        goto bail;
    }

    fprintf(stderr, "Audio capture started. Press Ctrl+C to stop.\n");
	exitStatus = 0;

	pthread_mutex_lock(&g_sleepMutex);
	pthread_cond_wait(&g_sleepCond, &g_sleepMutex);
	pthread_mutex_unlock(&g_sleepMutex);

    fprintf(stderr, "\nStopping audio capture...\n");
    g_deckLinkInput->StopStreams();
    g_deckLinkInput->DisableAudioInput();
	g_deckLinkInput->DisableVideoInput();


bail:
    if (g_ws_started) {
        if (g_ws_connected) {
            websocketpp::lib::error_code ec;
            g_ws_client.close(g_ws_hdl, websocketpp::close::status::going_away, "", ec);
            if (ec) {
                fprintf(stderr, "Error closing WebSocket connection: %s\n", ec.message().c_str());
            }
        }
        if (!g_ws_client.stopped()) {
            g_ws_client.stop();
        }
        pthread_join(g_ws_thread, NULL);
    }

    // cleanup_filter_graph() is no longer needed, the g_avectorscopeProcessor destructor handles it.

	if (displayMode != NULL)
					displayMode->Release();

	if (delegate != NULL)
						delegate->Release();

	if (g_deckLinkInput != NULL)
	{
							g_deckLinkInput->Release();
							g_deckLinkInput = NULL;
	}

	if (deckLinkAttributes != NULL)
								deckLinkAttributes->Release();

	if (deckLink != NULL)
								deckLink->Release();

	if (deckLinkIterator != NULL)
								deckLinkIterator->Release();

	return exitStatus;
}
