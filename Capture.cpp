/* -LICENSE-START-
** Copyright (c) 2013 Blackmagic Design
**  
** Permission is hereby granted, free of charge, to any person or organization 
** obtaining a copy of the software and accompanying documentation (the 
** "Software") to use, reproduce, display, distribute, sub-license, execute, 
** and transmit the Software, and to prepare derivative works of the Software, 
** and to permit third-parties to whom the Software is furnished to do so, in 
** accordance with:
** 
** (1) if the Software is obtained from Blackmagic Design, the End User License 
** Agreement for the Software Development Kit (“EULA”) available at 
** https://www.blackmagicdesign.com/EULA/DeckLinkSDK; or
** 
** (2) if the Software is obtained from any third party, such licensing terms 
** as notified by that third party,
** 
** and all subject to the following:
** 
** (3) the copyright notices in the Software and this entire statement, 
** including the above license grant, this restriction and the following 
** disclaimer, must be included in all copies of the Software, in whole or in 
** part, and all derivative works of the Software, unless such copies or 
** derivative works are solely in the form of machine-executable object code 
** generated by a source language processor.
** 
** (4) THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS 
** OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
** FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT 
** SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE 
** FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, 
** ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER 
** DEALINGS IN THE SOFTWARE.
** 
** A copy of the Software is available free of charge at 
** https://www.blackmagicdesign.com/desktopvideo_sdk under the EULA.
** 
** -LICENSE-END-
*/

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <pthread.h>
#include <unistd.h>
#include <fcntl.h>
#include <csignal>
#include <vector>
#include <numeric>
#include <sstream>

#include <websocketpp/config/asio_no_tls_client.hpp>
#include <websocketpp/client.hpp>

#include "DeckLinkAPI.h"
#include "Capture.h"
#include "Config.h"
#include "LKFS.h"
#include "base64.h"

extern "C" {
#include <libavfilter/avfilter.h>
#include <libavfilter/buffersink.h>
#include <libavfilter/buffersrc.h>
#include <libavutil/opt.h>
#include <libavutil/pixfmt.h>
#include <libavutil/pixdesc.h>
#include <libavutil/frame.h>
#include <libavutil/channel_layout.h>
}

// WebSocket client
typedef websocketpp::client<websocketpp::config::asio_client> client;
using websocketpp::lib::bind;
using websocketpp::lib::placeholders::_1;

static client g_ws_client;
static websocketpp::connection_hdl g_ws_hdl;
static bool g_ws_connected = false;
static pthread_mutex_t g_ws_mutex;
static pthread_t g_ws_thread;
static std::string g_ws_uri = "ws://127.0.0.1:8080";
static bool g_ws_started = false;
static bool		 g_do_exit = false;

void* ws_thread_func(void* /*arg*/)
{
    try {
        g_ws_client.run();
    } catch (const std::exception & e) {
        fprintf(stderr, "WebSocket thread exception: %s\n", e.what());
    } catch (websocketpp::lib::error_code e) {
        fprintf(stderr, "WebSocket thread error: %s\n", e.message().c_str());
    } catch (...) {
        fprintf(stderr, "WebSocket thread unknown exception.\n");
    }
    return NULL;
}

void send_ws_message(const std::string& msg) {
    if (g_do_exit) return;
    pthread_mutex_lock(&g_ws_mutex);
    if (g_ws_connected) {
        websocketpp::lib::error_code ec;
        g_ws_client.send(g_ws_hdl, msg, websocketpp::frame::opcode::text, ec);
        if (ec) {
            fprintf(stderr, "WebSocket send failed: %s\n", ec.message().c_str());
        }
    }
    pthread_mutex_unlock(&g_ws_mutex);
}

void on_ws_open(client* c, websocketpp::connection_hdl hdl) {
    pthread_mutex_lock(&g_ws_mutex);
    g_ws_connected = true;
    g_ws_hdl = hdl;
    pthread_mutex_unlock(&g_ws_mutex);
    fprintf(stderr, "WebSocket connection opened.\n");
    fflush(stderr);
}

void on_ws_close(client* c, websocketpp::connection_hdl hdl) {
    pthread_mutex_lock(&g_ws_mutex);
    g_ws_connected = false;
    pthread_mutex_unlock(&g_ws_mutex);
    fprintf(stderr, "WebSocket connection closed.\n");
    fflush(stderr);
}


// FFmpeg filter graph
static AVFilterGraph*   g_filterGraph = NULL;
static AVFilterContext* g_bufferSrcCtx = NULL;
static AVFilterContext* g_bufferSinkCtx = NULL;

// Audio processing globals
std::vector<double> g_leftChannelPcm;
std::vector<double> g_rightChannelPcm;

const int kAudioSampleRate = 48000;
const int kWindowSizeInSamples = kAudioSampleRate * 400 / 1000; // 19200
const int kSlideSizeInSamples = kAudioSampleRate * 100 / 1000;  // 4800

static pthread_mutex_t	 g_sleepMutex;
static pthread_cond_t	 g_sleepCond;
static int		 g_audioOutputFile = -1;

static BMDConfig		 g_config;

static IDeckLinkInput*		 g_deckLinkInput = NULL;

static void send_vectorscope_ws(AVFrame *pFrame) {
    // Create PPM header in memory
    std::stringstream ppm_stream;
    ppm_stream << "P6\n" << pFrame->width << " " << pFrame->height << "\n255\n";

    // Write pixel data to the stream
    for (int y = 0; y < pFrame->height; y++) {
        ppm_stream.write((char*)pFrame->data[0] + y * pFrame->linesize[0], pFrame->width * 3);
    }

    // Get the string from the stream
    std::string ppm_string = ppm_stream.str();

    // Base64 encode
    std::string encoded_data = base64_encode(reinterpret_cast<const unsigned char*>(ppm_string.c_str()), ppm_string.length());

    // Send via WebSocket
    std::ostringstream oss;
    oss << "{\"type\": \"vectorscope\", \"data\": \"" << encoded_data << "\"}";
    send_ws_message(oss.str());
}

static int init_filter_graph() {
    char args[512];
    int ret;

    const AVFilter *abuffersrc  = avfilter_get_by_name("abuffer");
    const AVFilter *avectorscope = avfilter_get_by_name("avectorscope");
    const AVFilter *format      = avfilter_get_by_name("format");
    const AVFilter *buffersink = avfilter_get_by_name("buffersink");

    AVFilterContext* avectorscope_ctx;
    AVFilterContext* format_ctx;

    g_filterGraph = avfilter_graph_alloc();
    if (!g_filterGraph) {
        fprintf(stderr, "Cannot allocate filter graph\n");
        return -1;
    }

    snprintf(args, sizeof(args), "time_base=1/%d:sample_rate=%d:sample_fmt=%s:channel_layout=stereo",
             kAudioSampleRate, kAudioSampleRate, av_get_sample_fmt_name(AV_SAMPLE_FMT_FLTP));
    ret = avfilter_graph_create_filter(&g_bufferSrcCtx, abuffersrc, "in", args, NULL, g_filterGraph);
    if (ret < 0) {
        fprintf(stderr, "Cannot create audio buffer source\n");
        return ret;
    }

    ret = avfilter_graph_create_filter(&avectorscope_ctx, avectorscope, "avectorscope", NULL, NULL, g_filterGraph);
    if (ret < 0) {
        fprintf(stderr, "Cannot create avectorscope filter\n");
        return ret;
    }
    av_opt_set_int(avectorscope_ctx, "mode", 1, 0); // Use 1 for lissajous_xy

    snprintf(args, sizeof(args), "pix_fmts=%s", av_get_pix_fmt_name(AV_PIX_FMT_RGB24));
    ret = avfilter_graph_create_filter(&format_ctx, format, "format", args, NULL, g_filterGraph);
    if (ret < 0) {
        fprintf(stderr, "Cannot create format filter\n");
        return ret;
    }

    ret = avfilter_graph_create_filter(&g_bufferSinkCtx, buffersink, "out", NULL, NULL, g_filterGraph);
    if (ret < 0) {
        fprintf(stderr, "Cannot create video buffer sink\n");
        return ret;
    }

    if ((ret = avfilter_link(g_bufferSrcCtx, 0, avectorscope_ctx, 0)) < 0) {
        fprintf(stderr, "Error linking abuffer to avectorscope: %d\n", ret);
        return ret;
    }
    if ((ret = avfilter_link(avectorscope_ctx, 0, format_ctx, 0)) < 0) {
        fprintf(stderr, "Error linking avectorscope to format: %d\n", ret);
        return ret;
    }
    if ((ret = avfilter_link(format_ctx, 0, g_bufferSinkCtx, 0)) < 0) {
        fprintf(stderr, "Error linking format to buffersink: %d\n", ret);
        return ret;
    }

    if ((ret = avfilter_graph_config(g_filterGraph, NULL)) < 0) {
        fprintf(stderr, "Error configuring the filter graph: %d\n", ret);
        return ret;
    }

    return 0;
}



static void cleanup_filter_graph() {
    if (g_filterGraph) {
        avfilter_graph_free(&g_filterGraph);
    }
}

DeckLinkCaptureDelegate::DeckLinkCaptureDelegate() :
	m_refCount(1)
{
}

ULONG DeckLinkCaptureDelegate::AddRef(void)
{
	return __sync_add_and_fetch(&m_refCount, 1);
}

ULONG DeckLinkCaptureDelegate::Release(void)
{
	int32_t newRefValue = __sync_sub_and_fetch(&m_refCount, 1);
	if (newRefValue == 0)
	{
		delete this;
		return 0;
	}
	return newRefValue;
}

HRESULT DeckLinkCaptureDelegate::VideoInputFrameArrived(IDeckLinkVideoInputFrame* videoFrame, IDeckLinkAudioInputPacket* audioFrame)
{
	(void)videoFrame; // Video frames are ignored, but used for timing

	if (audioFrame)
	{
        void* audioFrameBytes;
        audioFrame->GetBytes(&audioFrameBytes);
        const unsigned int sampleFrameCount = audioFrame->GetSampleFrameCount();
        const unsigned int channelCount = g_config.m_audioChannels;
        const unsigned int sampleDepth = g_config.m_audioSampleDepth;

        if (channelCount == 2)
        {
            // 1. Append new samples to global buffers ONCE.
            if (sampleDepth == 32)
            {
                int32_t* pcmData = (int32_t*)audioFrameBytes;
                for (unsigned int i = 0; i < sampleFrameCount; ++i)
                {
                    g_leftChannelPcm.push_back((double)pcmData[i * 2] / 2147483648.0);
                    g_rightChannelPcm.push_back((double)pcmData[i * 2 + 1] / 2147483648.0);
                }
            }
            else if (sampleDepth == 16)
            {
                int16_t* pcmData = (int16_t*)audioFrameBytes;
                for (unsigned int i = 0; i < sampleFrameCount; ++i)
                {
                    g_leftChannelPcm.push_back((double)pcmData[i * 2] / 32768.0);
                    g_rightChannelPcm.push_back((double)pcmData[i * 2 + 1] / 32768.0);
                }
            }

            // 2. Immediate vectorscope from the TAIL of the global buffers
            if (sampleFrameCount > 0)
            {
                // Create float vectors from the last 'sampleFrameCount' samples
                std::vector<float> leftChunk(g_leftChannelPcm.end() - sampleFrameCount, g_leftChannelPcm.end());
                std::vector<float> rightChunk(g_rightChannelPcm.end() - sampleFrameCount, g_rightChannelPcm.end());

                AVFrame *scope_frame = av_frame_alloc();
                if (scope_frame) {
                    scope_frame->sample_rate    = kAudioSampleRate;
                    scope_frame->format         = AV_SAMPLE_FMT_FLTP;
                    scope_frame->channel_layout = AV_CH_LAYOUT_STEREO;
                    scope_frame->nb_samples     = sampleFrameCount;

                    if (av_frame_get_buffer(scope_frame, 0) == 0) {
                        memcpy(scope_frame->data[0], leftChunk.data(), sampleFrameCount * sizeof(float));
                        memcpy(scope_frame->data[1], rightChunk.data(), sampleFrameCount * sizeof(float));

                        if (av_buffersrc_add_frame_flags(g_bufferSrcCtx, scope_frame, AV_BUFFERSRC_FLAG_KEEP_REF) >= 0) {
                            AVFrame *filt_frame = av_frame_alloc();
                            while (av_buffersink_get_frame(g_bufferSinkCtx, filt_frame) >= 0) {
                                if(filt_frame->width > 0) {
                                    send_vectorscope_ws(filt_frame);
                                }
                                av_frame_unref(filt_frame);
                            }
                            av_frame_free(&filt_frame);
                        }
                    }
                    av_frame_free(&scope_frame);
                }
            }

            // 3. Accumulated loudness calculation
            {
                while (g_leftChannelPcm.size() >= kWindowSizeInSamples)
                {
                    std::vector<double> leftWindow(g_leftChannelPcm.begin(), g_leftChannelPcm.begin() + kWindowSizeInSamples);
                    std::vector<double> rightWindow(g_rightChannelPcm.begin(), g_rightChannelPcm.begin() + kWindowSizeInSamples);

                    double lkfs = Momentary_loudness(leftWindow, rightWindow, kAudioSampleRate);
                    std::ostringstream oss;
                    oss << "{\"type\": \"lkfs\", \"value\": " << lkfs << "}";
                    send_ws_message(oss.str());

                    g_leftChannelPcm.erase(g_leftChannelPcm.begin(), g_leftChannelPcm.begin() + kSlideSizeInSamples);
                    g_rightChannelPcm.erase(g_rightChannelPcm.begin(), g_rightChannelPcm.begin() + kSlideSizeInSamples);
                }
            }
        }
	}
	return S_OK;
}


HRESULT DeckLinkCaptureDelegate::VideoInputFormatChanged(BMDVideoInputFormatChangedEvents /*events*/, IDeckLinkDisplayMode* /*mode*/, BMDDetectedVideoInputFormatFlags /*formatFlags*/)
{
	return S_OK;
}

static void sigfunc(int signum)
{
	if (signum == SIGINT || signum == SIGTERM)
			g_do_exit = true;

	pthread_cond_signal(&g_sleepCond);
}

int main(int argc, char *argv[])
{
	HRESULT			 result;
	int			 exitStatus = 1;

	IDeckLinkIterator*		 deckLinkIterator = NULL;
	IDeckLink*			 deckLink = NULL;

	IDeckLinkProfileAttributes*	 deckLinkAttributes = NULL;
	int64_t			 duplexMode;

	IDeckLinkDisplayMode*		 displayMode = NULL;

	DeckLinkCaptureDelegate*		 delegate = NULL;

	pthread_mutex_init(&g_sleepMutex, NULL);
	pthread_cond_init(&g_sleepCond, NULL);
	pthread_mutex_init(&g_ws_mutex, NULL);

	signal(SIGINT, sigfunc);
	signal(SIGTERM, sigfunc);
	signal(SIGHUP, sigfunc);

	if (!g_config.ParseArguments(argc, argv))
	{
		g_config.DisplayUsage(exitStatus);
		goto bail;
	}

    // WebSocket client setup
    try {
        g_ws_client.clear_access_channels(websocketpp::log::alevel::all);
        g_ws_client.set_access_channels(websocketpp::log::alevel::connect | websocketpp::log::alevel::disconnect);
        g_ws_client.set_error_channels(websocketpp::log::elevel::all);
        g_ws_client.init_asio();

        g_ws_client.set_open_handler(bind(&on_ws_open, &g_ws_client, ::_1));
        g_ws_client.set_close_handler(bind(&on_ws_close, &g_ws_client, ::_1));

        websocketpp::lib::error_code ec;
        client::connection_ptr con = g_ws_client.get_connection(g_ws_uri, ec);
        if (ec) {
            fprintf(stderr, "Could not create connection: %s\n", ec.message().c_str());
            goto bail;
        }

        g_ws_client.connect(con);

        pthread_create(&g_ws_thread, NULL, ws_thread_func, NULL);
        g_ws_started = true;

    } catch (const std::exception & e) {
        fprintf(stderr, "WebSocket setup exception: %s\n", e.what());
        goto bail;
    } catch (websocketpp::lib::error_code e) {
        fprintf(stderr, "WebSocket setup error: %s\n", e.message().c_str());
        goto bail;
    } catch (...) {
        fprintf(stderr, "WebSocket setup unknown exception.\n");
        goto bail;
    }

	if (init_filter_graph() < 0) {
		fprintf(stderr, "Failed to initialize filter graph\n");
		goto bail;
	}

	deckLink = g_config.GetSelectedDeckLink();
	if (deckLink == NULL)
	{
		fprintf(stderr, "Unable to get DeckLink device %u\n", g_config.m_deckLinkIndex);
		goto bail;
	}

	result = deckLink->QueryInterface(IID_IDeckLinkInput, (void**)&g_deckLinkInput);
	if (result != S_OK)
	{
		fprintf(stderr, "The selected device does not have an input interface\n");
		goto bail;
	}

	result = deckLink->QueryInterface(IID_IDeckLinkProfileAttributes, (void**)&deckLinkAttributes);
	if (result != S_OK)
	{
		fprintf(stderr, "Unable to get DeckLink attributes interface\n");
		goto bail;
	}
	result = deckLinkAttributes->GetInt(BMDDeckLinkDuplex, &duplexMode);
	if ((result != S_OK) || (duplexMode == bmdDuplexInactive))
	{
		fprintf(stderr, "The selected DeckLink device is inactive\n");
		goto bail;
	}

    bool formatDetectionSupported;
    result = deckLinkAttributes->GetFlag(BMDDeckLinkSupportsInputFormatDetection, &formatDetectionSupported);
    if (result == S_OK && formatDetectionSupported)
    {
        g_config.m_inputFlags |= bmdVideoInputEnableFormatDetection;
    }

    displayMode = g_config.GetSelectedDeckLinkDisplayMode(deckLink);
    if (displayMode == NULL)
    {
        fprintf(stderr, "Error: Could not find a valid display mode.\n");
        goto bail;
    }

	delegate = new DeckLinkCaptureDelegate();
	g_deckLinkInput->SetCallback(delegate);

	if (g_config.m_audioOutputFile != NULL)
	{
		g_audioOutputFile = open(g_config.m_audioOutputFile, O_WRONLY|O_CREAT|O_TRUNC, 0664);
		if (g_audioOutputFile < 0)
		{
			fprintf(stderr, "Could not open audio output file \"%s\"\n", g_config.m_audioOutputFile);
			goto bail;
		}
	}

    result = g_deckLinkInput->EnableVideoInput(displayMode->GetDisplayMode(), bmdFormat8BitYUV, g_config.m_inputFlags);
    if (result != S_OK)
    {
        fprintf(stderr, "Failed to enable video input. Is a video signal connected?\n");
        goto bail;
    }

    result = g_deckLinkInput->EnableAudioInput(bmdAudioSampleRate48kHz, g_config.m_audioSampleDepth, g_config.m_audioChannels);
    if (result != S_OK)
    {
        fprintf(stderr, "Failed to enable audio input.\n");
        goto bail;
    }

    result = g_deckLinkInput->StartStreams();
    if (result != S_OK)
    {
        fprintf(stderr, "Failed to start streams.\n");
        goto bail;
    }

    fprintf(stderr, "Audio capture started. Press Ctrl+C to stop.\n");
	exitStatus = 0;

	pthread_mutex_lock(&g_sleepMutex);
	pthread_cond_wait(&g_sleepCond, &g_sleepMutex);
	pthread_mutex_unlock(&g_sleepMutex);

    fprintf(stderr, "\nStopping audio capture...\n");
    g_deckLinkInput->StopStreams();
    g_deckLinkInput->DisableAudioInput();
	g_deckLinkInput->DisableVideoInput();


bail:
    if (g_ws_started) {
        if (g_ws_connected) {
            websocketpp::lib::error_code ec;
            g_ws_client.close(g_ws_hdl, websocketpp::close::status::going_away, "", ec);
            if (ec) {
                fprintf(stderr, "Error closing WebSocket connection: %s\n", ec.message().c_str());
            }
        }
        if (!g_ws_client.stopped()) {
            g_ws_client.stop();
        }
        pthread_join(g_ws_thread, NULL);
    }

    cleanup_filter_graph();

	if (g_audioOutputFile != 0)
		close(g_audioOutputFile);

	if (displayMode != NULL)
		displayMode->Release();

	if (delegate != NULL)
		delegate->Release();

	if (g_deckLinkInput != NULL)
	{
		g_deckLinkInput->Release();
		g_deckLinkInput = NULL;
	}

	if (deckLinkAttributes != NULL)
		deckLinkAttributes->Release();

	if (deckLink != NULL)
		deckLink->Release();

	if (deckLinkIterator != NULL)
		deckLinkIterator->Release();

	return exitStatus;
}
