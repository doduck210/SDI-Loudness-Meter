/* -LICENSE-START-
** Copyright (c) 2013 Blackmagic Design
**
** Permission is hereby granted, free of charge, to any person or organization
** obtaining a copy of the software and accompanying documentation (the
** "Software") to use, reproduce, display, distribute, sub-license, execute,
** and transmit the Software, and to prepare derivative works of the Software,
** and to permit third-parties to whom the Software is furnished to do so, in
** accordance with:
**
** (1) if the Software is obtained from Blackmagic Design, the End User License
** Agreement for the Software Development Kit (“EULA”) available at
** https://www.blackmagicdesign.com/EULA/DeckLinkSDK; or
**
** (2) if the Software is obtained from any third party, such licensing terms
** as notified by that third party,
**
** and all subject to the following:
**
** (3) the copyright notices in the Software and this entire statement,
** including the above license grant, this restriction and the following
** disclaimer, must be included in all copies of the Software, in whole or in
** part, and all derivative works of the Software, unless such copies or
** derivative works are solely in the form of machine-executable object code
** generated by a source language processor.
**
** (4) THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
** OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
** FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
** SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
** FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
** ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
** DEALINGS IN THE SOFTWARE.
**
** A copy of the Software is available free of charge at
** https://www.blackmagicdesign.com/desktopvideo_sdk under the EULA.
**
** -LICENSE-END-
*/

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <pthread.h>
#include <unistd.h>
#include <fcntl.h>
#include <csignal>
#include <vector>
#include <deque>
#include <numeric>
#include <sstream>

#include <websocketpp/config/asio_no_tls_client.hpp>
#include <websocketpp/client.hpp>

#include "DeckLinkAPI.h"
#include "Capture.h"
#include "Config.h"
#include "LKFS.h"
#include "avectorscope_processor.h" // Renamed for clarity
#include "eq_processor.h"           // For EQ Meter processing
#include "correlator_processor.h"   // For audio correlation processing

#ifdef ENABLE_VIDEO_PROCESSING
#include "VideoProcessor.h"
#endif

// WebSocket client
typedef websocketpp::client<websocketpp::config::asio_client> client;
using websocketpp::lib::bind;
using websocketpp::lib::placeholders::_1, websocketpp::lib::placeholders::_2;

static client g_ws_client;
static websocketpp::connection_hdl g_ws_hdl;
static bool g_ws_connected = false;
static pthread_mutex_t g_ws_mutex;
static pthread_t g_ws_thread;
static std::string g_ws_uri = "ws://127.0.0.1:8080";
static bool g_ws_started = false;
static bool		 g_do_exit = false;
static bool g_isIntegrating = false;

// FFmpeg-related globals
static AVectorscopeProcessor g_avectorscopeProcessor;
static EQProcessor g_eqProcessor;
static CorrelatorProcessor g_correlatorProcessor;
#ifdef ENABLE_VIDEO_PROCESSING
static VideoProcessor g_videoProcessor;
#endif

// Audio processing globals
std::deque<double> g_leftChannelPcm;
std::deque<double> g_rightChannelPcm;
std::deque<double> g_shortTermLeftChannelPcm;
std::deque<double> g_shortTermRightChannelPcm;
std::vector<double> g_momentaryLoudnessHistory;

const int kAudioSampleRate = 48000;
const int kWindowSizeInSamples = kAudioSampleRate * 400 / 1000; // 19200
const int kShortTermWindowSizeInSamples = kAudioSampleRate * 3; // 144000
const int kSlideSizeInSamples = kAudioSampleRate * 100 / 1000;  // 4800

static pthread_mutex_t	 g_sleepMutex;
static pthread_cond_t	 g_sleepCond;
static BMDConfig		 g_config;
static IDeckLinkInput*		 g_deckLinkInput = NULL;

void* ws_thread_func(void* /*arg*/) {
    try {
        g_ws_client.run();
    } catch (const std::exception & e) {
        fprintf(stderr, "WebSocket thread exception: %s\n", e.what());
    } catch (websocketpp::lib::error_code e) {
        fprintf(stderr, "WebSocket thread error: %s\n", e.message().c_str());
    } catch (...) {
        fprintf(stderr, "WebSocket thread unknown exception.\n");
    }
    return NULL;
}

void send_ws_message(const std::string& msg) {
    if (g_do_exit) return;
    pthread_mutex_lock(&g_ws_mutex);
    if (g_ws_connected) {
        websocketpp::lib::error_code ec;
        g_ws_client.send(g_ws_hdl, msg, websocketpp::frame::opcode::text, ec);
        if (ec) {
            fprintf(stderr, "WebSocket send failed: %s\n", ec.message().c_str());
        }
    }
    pthread_mutex_unlock(&g_ws_mutex);
}

void on_ws_open(client* c, websocketpp::connection_hdl hdl) {
    pthread_mutex_lock(&g_ws_mutex);
    g_ws_connected = true;
    g_ws_hdl = hdl;
    pthread_mutex_unlock(&g_ws_mutex);
    fprintf(stderr, "WebSocket connection opened.\n");
    fflush(stderr);
}

void on_ws_close(client* c, websocketpp::connection_hdl hdl) {
    pthread_mutex_lock(&g_ws_mutex);
    g_ws_connected = false;
    pthread_mutex_unlock(&g_ws_mutex);
    fprintf(stderr, "WebSocket connection closed.\n");
    fflush(stderr);
}

void on_ws_message(client* c, websocketpp::connection_hdl hdl, client::message_ptr msg) {
    std::string payload = msg->get_payload();
    if (payload.find("\"command\"") != std::string::npos && payload.find("\"start_integration\"") != std::string::npos) {
        fprintf(stderr, "Received start integration command.\n");
        g_momentaryLoudnessHistory.clear();
        g_isIntegrating = true;
    } else if (payload.find("\"command\"") != std::string::npos && payload.find("\"stop_integration\"") != std::string::npos) {
        fprintf(stderr, "Received stop integration command.\n");
        g_isIntegrating = false;
    }
}

DeckLinkCaptureDelegate::DeckLinkCaptureDelegate() :
	m_refCount(1),
    m_pixelFormat(g_config.m_pixelFormat)
{
}

ULONG DeckLinkCaptureDelegate::AddRef(void)
{
	return __sync_add_and_fetch(&m_refCount, 1);
}

ULONG DeckLinkCaptureDelegate::Release(void)
{
	int32_t newRefValue = __sync_sub_and_fetch(&m_refCount, 1);
	if (newRefValue == 0)
	{
		delete this;
		return 0;
	}
	return newRefValue;
}

HRESULT DeckLinkCaptureDelegate::VideoInputFrameArrived(IDeckLinkVideoInputFrame* videoFrame, IDeckLinkAudioInputPacket* audioFrame)
{
    if (videoFrame) {
        if (videoFrame->GetFlags() & bmdFrameHasNoInputSource) {
            fprintf(stderr, "No input signal detected\n");
        } else {
            #ifdef ENABLE_VIDEO_PROCESSING
            g_videoProcessor.processFrame(videoFrame, [](const std::string& msg) {
                // We don't send video data over websocket, so this is a no-op.
            });
            #endif
        }
    }

    if (audioFrame)
    {
        void* audioFrameBytes;
        audioFrame->GetBytes(&audioFrameBytes);
        const unsigned int sampleFrameCount = audioFrame->GetSampleFrameCount();
        const unsigned int channelCount = g_config.m_audioChannels;
        const unsigned int sampleDepth = g_config.m_audioSampleDepth;

        if (channelCount == 2)
        {
            double maxLeft = 0.0;
            double maxRight = 0.0;
            std::vector<double> current_left_samples;
            std::vector<double> current_right_samples;
            current_left_samples.reserve(sampleFrameCount);
            current_right_samples.reserve(sampleFrameCount);

            if (sampleDepth == 32)
            {
                int32_t* pcmData = (int32_t*)audioFrameBytes;
                for (unsigned int i = 0; i < sampleFrameCount; ++i)
                {
                    double leftSample = (double)pcmData[i * 2] / 2147483648.0;
                    double rightSample = (double)pcmData[i * 2 + 1] / 2147483648.0;
                    if (std::abs(leftSample) > maxLeft) maxLeft = std::abs(leftSample);
                    if (std::abs(rightSample) > maxRight) maxRight = std::abs(rightSample);
                    g_leftChannelPcm.push_back(leftSample);
                    g_rightChannelPcm.push_back(rightSample);
                    g_shortTermLeftChannelPcm.push_back(leftSample);
                    g_shortTermRightChannelPcm.push_back(rightSample);
                    current_left_samples.push_back(leftSample);
                    current_right_samples.push_back(rightSample);
                }
            }
            else if (sampleDepth == 16)
            {
                int16_t* pcmData = (int16_t*)audioFrameBytes;
                for (unsigned int i = 0; i < sampleFrameCount; ++i)
                {
                    double leftSample = (double)pcmData[i * 2] / 32768.0;
                    double rightSample = (double)pcmData[i * 2 + 1] / 32768.0;
                    if (std::abs(leftSample) > maxLeft) maxLeft = std::abs(leftSample);
                    if (std::abs(rightSample) > maxRight) maxRight = std::abs(rightSample);
                    g_leftChannelPcm.push_back(leftSample);
                    g_rightChannelPcm.push_back(rightSample);
                    g_shortTermLeftChannelPcm.push_back(leftSample);
                    g_shortTermRightChannelPcm.push_back(rightSample);
                    current_left_samples.push_back(leftSample);
                    current_right_samples.push_back(rightSample);
                }
            }

            double leftDb = (maxLeft > 0.0) ? (20.0 * log10(maxLeft)) : -100.0;
            double rightDb = (maxRight > 0.0) ? (20.0 * log10(maxRight)) : -100.0;
            std::ostringstream oss_levels;
            oss_levels << "{\"type\": \"levels\", \"left\": " << leftDb << ", \"right\": " << rightDb << "}";
            send_ws_message(oss_levels.str());

            if (sampleFrameCount > 0)
            {
                std::vector<float> leftChunk(g_leftChannelPcm.end() - sampleFrameCount, g_leftChannelPcm.end());
                std::vector<float> rightChunk(g_rightChannelPcm.end() - sampleFrameCount, g_rightChannelPcm.end());
                g_avectorscopeProcessor.processAudio(leftChunk, rightChunk, sampleFrameCount,
                    [](const std::string& msg) { send_ws_message(msg); });
            }

            while (g_leftChannelPcm.size() >= kWindowSizeInSamples)
            {
                std::vector<double> leftWindow(g_leftChannelPcm.begin(), g_leftChannelPcm.begin() + kWindowSizeInSamples);
                std::vector<double> rightWindow(g_rightChannelPcm.begin(), g_rightChannelPcm.begin() + kWindowSizeInSamples);
                double lkfs = Momentary_loudness(leftWindow, rightWindow, kAudioSampleRate);
                std::ostringstream oss;
                oss << "{\"type\": \"lkfs\", \"value\": " << lkfs << "}";
                send_ws_message(oss.str());
                if(g_isIntegrating){
                    g_momentaryLoudnessHistory.push_back(lkfs);
                    double i_lkfs = integrated_loudness_with_momentaries(g_momentaryLoudnessHistory, kAudioSampleRate);
                    std::ostringstream oss_i;
                    oss_i << "{\"type\": \"i_lkfs\", \"value\": " << i_lkfs << "}";
                    send_ws_message(oss_i.str());
                }
                for (unsigned int i = 0; i < kSlideSizeInSamples; ++i) {
                    g_leftChannelPcm.pop_front();
                    g_rightChannelPcm.pop_front();
                }
            }

            while (g_shortTermLeftChannelPcm.size() >= kShortTermWindowSizeInSamples)
            {
                std::vector<double> leftWindow(g_shortTermLeftChannelPcm.begin(), g_shortTermLeftChannelPcm.begin() + kShortTermWindowSizeInSamples);
                std::vector<double> rightWindow(g_shortTermRightChannelPcm.begin(), g_shortTermRightChannelPcm.begin() + kShortTermWindowSizeInSamples);
                double s_lkfs = ShortTerm_loudness(leftWindow, rightWindow, kAudioSampleRate);
                std::ostringstream oss_s;
                oss_s << "{\"type\": \"s_lkfs\", \"value\": " << s_lkfs << "}";
                send_ws_message(oss_s.str());
                for (unsigned int i = 0; i < kSlideSizeInSamples; ++i) {
                    g_shortTermLeftChannelPcm.pop_front();
                    g_shortTermRightChannelPcm.pop_front();
                }
            }

            if (sampleFrameCount > 0)
            {
                // Calculate and send correlation
                std::vector<float> left_float(current_left_samples.begin(), current_left_samples.end());
                std::vector<float> right_float(current_right_samples.begin(), current_right_samples.end());
                float correlation = g_correlatorProcessor.process(left_float.data(), right_float.data(), sampleFrameCount);
                std::ostringstream oss_corr;
                oss_corr << "{\"type\": \"correlation\", \"value\": " << correlation << "}";
                send_ws_message(oss_corr.str());

                g_eqProcessor.processAudio(current_left_samples.data(), current_right_samples.data(), sampleFrameCount,
                    [](const std::string& msg) { send_ws_message(msg); });
            }
        }
    }
	return S_OK;
}

HRESULT DeckLinkCaptureDelegate::VideoInputFormatChanged(BMDVideoInputFormatChangedEvents events, IDeckLinkDisplayMode* mode, BMDDetectedVideoInputFormatFlags formatFlags)
{
	HRESULT	result;
	char* displayModeName = NULL;
	BMDPixelFormat pixelFormat = m_pixelFormat;

	if (events & bmdVideoInputColorspaceChanged)
	{
		if (formatFlags & bmdDetectedVideoInputRGB444)
			pixelFormat = bmdFormat10BitRGB;
		else if (formatFlags & bmdDetectedVideoInputYCbCr422)
			pixelFormat = (g_config.m_pixelFormat == bmdFormat8BitYUV) ? bmdFormat8BitYUV : bmdFormat10BitYUV;
		else
			goto bail;
	}

	if ((events & bmdVideoInputDisplayModeChanged) || (m_pixelFormat != pixelFormat))
	{
		mode->GetName((const char**)&displayModeName);
		printf("Video format changed to %s %s\n", displayModeName, formatFlags & bmdDetectedVideoInputRGB444 ? "RGB" : "YUV");

		if (displayModeName)
			free(displayModeName);

		if (g_deckLinkInput)
		{
			g_deckLinkInput->StopStreams();
			result = g_deckLinkInput->EnableVideoInput(mode->GetDisplayMode(), pixelFormat, g_config.m_inputFlags);
			if (result != S_OK)
			{
				fprintf(stderr, "Failed to switch video mode\n");
				goto bail;
			}

			g_deckLinkInput->StartStreams();
		}
		m_pixelFormat = pixelFormat;
	}

bail:
	return S_OK;
}

static void sigfunc(int signum)
{
	if (signum == SIGINT || signum == SIGTERM)
		 g_do_exit = true;

	pthread_cond_signal(&g_sleepCond);
}

int main(int argc, char *argv[])
{
	HRESULT result;
	int exitStatus = 1;
	IDeckLinkIterator* deckLinkIterator = NULL;
	IDeckLink* deckLink = NULL;
	IDeckLinkProfileAttributes* deckLinkAttributes = NULL;
	bool formatDetectionSupported;
	IDeckLinkDisplayMode* displayMode = NULL;
	DeckLinkCaptureDelegate* delegate = NULL;

	pthread_mutex_init(&g_sleepMutex, NULL);
	pthread_cond_init(&g_sleepCond, NULL);
	pthread_mutex_init(&g_ws_mutex, NULL);

	signal(SIGINT, sigfunc);
	signal(SIGTERM, sigfunc);
	signal(SIGHUP, sigfunc);

	if (!g_config.ParseArguments(argc, argv))
	{
		g_config.DisplayUsage(exitStatus);
		goto bail;
	}

    try {
        g_ws_client.clear_access_channels(websocketpp::log::alevel::all);
        g_ws_client.set_access_channels(websocketpp::log::alevel::connect | websocketpp::log::alevel::disconnect);
        g_ws_client.set_error_channels(websocketpp::log::elevel::all);
        g_ws_client.init_asio();
        g_ws_client.set_open_handler(bind(&on_ws_open, &g_ws_client, ::_1));
        g_ws_client.set_close_handler(bind(&on_ws_close, &g_ws_client, ::_1));
        g_ws_client.set_message_handler(bind(&on_ws_message, &g_ws_client, ::_1, ::_2));
        websocketpp::lib::error_code ec;
        client::connection_ptr con = g_ws_client.get_connection(g_ws_uri, ec);
        if (ec) { fprintf(stderr, "Could not create connection: %s\n", ec.message().c_str()); goto bail; }
        g_ws_client.connect(con);
        pthread_create(&g_ws_thread, NULL, ws_thread_func, NULL);
        g_ws_started = true;
    } catch (const std::exception & e) {
        fprintf(stderr, "WebSocket setup exception: %s\n", e.what());
        goto bail;
    } catch (...) {
        fprintf(stderr, "WebSocket setup unknown exception.\n");
        goto bail;
    }

	if (!g_avectorscopeProcessor.initialize()) { fprintf(stderr, "Failed to initialize vectorscope processor\n"); goto bail; }
	g_eqProcessor.initialize();

	deckLink = g_config.GetSelectedDeckLink();
	if (deckLink == NULL) { fprintf(stderr, "Unable to get DeckLink device %u\n", g_config.m_deckLinkIndex); goto bail; }

	if (deckLink->QueryInterface(IID_IDeckLinkInput, (void**)&g_deckLinkInput) != S_OK) { fprintf(stderr, "The selected device does not have an input interface\n"); goto bail; }

    if (g_config.m_displayModeIndex == -1) {
        if (deckLink->QueryInterface(IID_IDeckLinkProfileAttributes, (void**)&deckLinkAttributes) == S_OK) {
            if (deckLinkAttributes->GetFlag(BMDDeckLinkSupportsInputFormatDetection, &formatDetectionSupported) == S_OK && formatDetectionSupported) {
                g_config.m_inputFlags |= bmdVideoInputEnableFormatDetection;
            }
            deckLinkAttributes->Release();
        }
    }

    displayMode = g_config.GetSelectedDeckLinkDisplayMode(deckLink);
    if (displayMode == NULL) { fprintf(stderr, "Error: Could not find a valid display mode.\n"); goto bail; }

	delegate = new DeckLinkCaptureDelegate();
	g_deckLinkInput->SetCallback(delegate);

    #ifdef ENABLE_VIDEO_PROCESSING
    fprintf(stderr, "Video processing is enabled.\n");
    #endif

    // Start capturing
    while (!g_do_exit) {
        result = g_deckLinkInput->EnableVideoInput(displayMode->GetDisplayMode(), g_config.m_pixelFormat, g_config.m_inputFlags);
        if (result != S_OK) { fprintf(stderr, "Failed to enable video input. Is a video signal connected?\n"); goto bail; }

        #ifdef ENABLE_VIDEO_PROCESSING
        BMDTimeValue timeScale, frameDuration;
        displayMode->GetFrameRate(&frameDuration, &timeScale);
        if (!g_videoProcessor.initialize(displayMode->GetWidth(), displayMode->GetHeight(), timeScale, frameDuration, g_config.m_pixelFormat)) {
            fprintf(stderr, "Failed to initialize video processor\n");
            goto bail;
        }
        #endif

        result = g_deckLinkInput->EnableAudioInput(bmdAudioSampleRate48kHz, g_config.m_audioSampleDepth, g_config.m_audioChannels);
        if (result != S_OK) { fprintf(stderr, "Failed to enable audio input.\n"); goto bail; }

        result = g_deckLinkInput->StartStreams();
        if (result != S_OK) { fprintf(stderr, "Failed to start streams.\n"); goto bail; }

        fprintf(stderr, "Capture started. Press Ctrl+C to stop.\n");
        exitStatus = 0;

        pthread_mutex_lock(&g_sleepMutex);
        pthread_cond_wait(&g_sleepCond, &g_sleepMutex);
        pthread_mutex_unlock(&g_sleepMutex);

        fprintf(stderr, "\nStopping capture...\n");
        g_deckLinkInput->StopStreams();
        g_deckLinkInput->DisableAudioInput();
        g_deckLinkInput->DisableVideoInput();
    }


bail:
    if (g_ws_started) {
        if (g_ws_connected) {
            websocketpp::lib::error_code ec;
            g_ws_client.close(g_ws_hdl, websocketpp::close::status::going_away, "", ec);
        }
        if (!g_ws_client.stopped()) g_ws_client.stop();
        pthread_join(g_ws_thread, NULL);
    }

	if (displayMode != NULL) displayMode->Release();
	if (delegate != NULL) delegate->Release();
	if (g_deckLinkInput != NULL) { g_deckLinkInput->Release(); g_deckLinkInput = NULL; }
	if (deckLink != NULL) deckLink->Release();

	return exitStatus;
}